{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the learning I have accumalted from sources like blogs and ChatGPT about the working of autograd (aka Automatic Gradient Differentiation)\n",
    "\n",
    "- PyTorch is a deep learning framework that uses automatic differentiation to compute gradients. The gradients are used to optimize the parameters of a neural network during the training process.\n",
    "\n",
    "- The process of computing gradients in PyTorch using automatic differentiation is called \"torch.autograd\" or simply \"autograd\". The autograd package in PyTorch provides classes and functions to compute gradients in a dynamic computational graph.\n",
    "\n",
    "Here is an overview of how autograd works in PyTorch:\n",
    "\n",
    "- - _Define a computational graph_: A computational graph is a directed acyclic graph (DAG) that describes the computation of a neural network. Each node in the graph represents a tensor and each edge represents the operation that produces the tensor.\n",
    "\n",
    "- - _Initialize tensors_: To compute gradients, the tensors in the graph must be initialized with the \"requires_grad\" attribute set to \"True\". This tells PyTorch to keep track of the operations performed on the tensor and to calculate the gradients.\n",
    "\n",
    "- - _Perform operations_: The operations on the tensors are performed in the forward pass of the neural network. The operations can include simple arithmetic operations, convolutional layers, and others.\n",
    "\n",
    "- - _Compute gradients_: After the forward pass is complete, the gradients can be computed by calling the \"backward()\" function on the tensor that represents the loss. The gradients are calculated by applying the chain rule of calculus to the operations in the computational graph.\n",
    "\n",
    "- - _Update parameters_: The gradients can be used to update the parameters of the neural network using optimization algorithms such as stochastic gradient descent (SGD) or Adam.\n",
    "\n",
    "- - _Clear gradients_: It's important to call the \"zero_grad()\" function before each forward pass, this will clear the gradients from the previous iteration, so that it doesn't accumulate.\n",
    "\n",
    "- It is worth noting that PyTorch uses a dynamic computational graph, which means that the graph is constructed on the fly as the operations are performed, and it is also possible to change the graph during the execution. This allows for flexibility and efficient memory usage.\n",
    "\n",
    "- Additionally, PyTorch also allows you to define custom autograd functions by subclassing the \"torch.autograd.Function\" class and implementing the forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
